{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d89a03f5-4680-43b2-8198-5efded84811c",
   "metadata": {},
   "source": [
    "üîß 1. Install & Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a810647c-22be-4c7d-947d-468ef58dd073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using device: cpu\n",
      "Torch version: 2.9.1+cpu\n",
      "CUDA available: False\n",
      "CUDA version (compiled): None\n",
      "GPU count: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import timm\n",
    "import os\n",
    "\n",
    "# Device setup\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"‚úÖ Using device:\", device)\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version (compiled):\", torch.version.cuda)\n",
    "print(\"GPU count:\", torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163f3115-4444-4ade-acd4-2294f95c9979",
   "metadata": {},
   "source": [
    "üß† 2. Load DINOv2 & Smart Unfreezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18208d1d-b492-487d-96f3-93edd563e34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Load pretrained DINOv2 ViT-Base model\n",
    "backbone = timm.create_model('vit_base_patch14_dinov2.lvd142m', pretrained=True, num_classes=0).to(device)\n",
    "\n",
    "# Freeze entire model initially\n",
    "for param in backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# ‚úÖ Smart unfreezing: last transformer block and final norm layer\n",
    "if hasattr(backbone, 'blocks'):\n",
    "    for param in backbone.blocks[-1].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "if hasattr(backbone, 'norm'):\n",
    "    for param in backbone.norm.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# ‚úÖ Print expected input size\n",
    "print(\"‚úÖ Input size expected by DINOv2:\", backbone.default_cfg['input_size'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b92d44f-83f1-4784-8320-467983caa140",
   "metadata": {},
   "source": [
    "üèóÔ∏è 3. Define Classifier Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918381c0-fa2b-441c-a338-8c7a5363e504",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DinoClassifier(nn.Module):\n",
    "    def __init__(self, backbone, num_classes):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(768, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.backbone(x)\n",
    "        return self.head(feats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c356993-415e-41ab-9f63-78b965e57391",
   "metadata": {},
   "source": [
    "üßº 4. Transforms & Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db511e69-9a63-4d0a-ab56-03c482ec37c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Local dataset root\n",
    "data_path = r\"D:\\Dermora\\dataset\"\n",
    "\n",
    "# ‚úÖ Training data augmentation\n",
    "train_transform = T.Compose([\n",
    "    T.RandomResizedCrop(518, scale=(0.8, 1.0)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomRotation(10),\n",
    "    T.ColorJitter(0.2, 0.2, 0.2),\n",
    "    T.GaussianBlur(kernel_size=3),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406],\n",
    "                [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ‚úÖ Validation/test transforms (no augmentation)\n",
    "test_transform = T.Compose([\n",
    "    T.Resize((518, 518)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406],\n",
    "                [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ‚úÖ Load datasets using correct Windows path\n",
    "train_dataset = ImageFolder(os.path.join(data_path, 'train'), transform=train_transform)\n",
    "val_dataset   = ImageFolder(os.path.join(data_path, 'val'), transform=test_transform)\n",
    "test_dataset  = ImageFolder(os.path.join(data_path, 'test'), transform=test_transform)\n",
    "\n",
    "# ‚úÖ Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True,\n",
    "                          num_workers=2, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=16, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=16, num_workers=2, pin_memory=True)\n",
    "\n",
    "# ‚úÖ Class names\n",
    "class_names = train_dataset.classes\n",
    "print(\"‚úÖ Classes:\", class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2035c5-a0f0-4b46-aa48-624455243fa9",
   "metadata": {},
   "source": [
    "‚öôÔ∏è 5. Loss, Optimizer, Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c6a0cd-d1cf-4319-aa03-b887985b4b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming backbone is already defined in Block 2\n",
    "model = DinoClassifier(backbone, num_classes=3).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=2, factor=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0fc27f-355a-4be6-b054-6a34874b6c36",
   "metadata": {},
   "source": [
    "üîÅ 6. Training, Validation, Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50be86cf-6602-45a0-aa50-64879b650b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train(model, loader, epoch):\n",
    "    model.train()\n",
    "    total, correct, running_loss = 0, 0, 0\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"üîÅ Training (Epoch {epoch})\")\n",
    "    for images, labels in pbar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        pbar.set_postfix(loss=loss.item(), acc=correct / total)\n",
    "\n",
    "    return correct / total, running_loss / len(loader)\n",
    "\n",
    "\n",
    "def validate(model, loader, epoch):\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"üß™ Validating (Epoch {epoch})\")\n",
    "    with torch.no_grad():\n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            pbar.set_postfix(acc=correct / total)\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=\"üîç Evaluating\"):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = outputs.argmax(dim=1).cpu()\n",
    "            all_preds.extend(preds.numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "    print(\"\\nüìä Classification Report:\\n\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e798ee-9329-415c-89e7-8597d61e2f76",
   "metadata": {},
   "source": [
    "üß† 7. Training Loop with Full Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c10a181-4ad4-4837-8680-0e6bcf84f623",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    print(f\"\\nüìÖ Epoch {epoch}/{num_epochs}\")\n",
    "\n",
    "    train_acc, train_loss = train(model, train_loader, epoch)\n",
    "    val_acc = validate(model, val_loader, epoch)\n",
    "    scheduler.step(val_acc)\n",
    "\n",
    "    print(f\"‚úÖ Train Acc: {train_acc:.4f} | Loss: {train_loss:.4f}\")\n",
    "    print(f\"üß™ Val   Acc: {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_acc': best_acc\n",
    "        }, 'best_dino_checkpoint.pth')\n",
    "        print(\"üíæ Model checkpoint saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10517a5f-d371-48b1-a7b7-2f70e9fef3b4",
   "metadata": {},
   "source": [
    "üíæ 8. Load Best Model & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5074837-ba6e-44d1-a08b-3e3eea132a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üíæ 8. Load Best Model & Evaluate\n",
    "\n",
    "# Load checkpoint from file\n",
    "checkpoint_path = 'best_dino_checkpoint.pth'\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "# Load weights into model and optimizer\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "best_epoch = checkpoint['epoch']\n",
    "best_val_acc = checkpoint['best_acc']\n",
    "\n",
    "print(f\"‚úÖ Loaded checkpoint from epoch {best_epoch} with best val accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "# Final test evaluation\n",
    "evaluate(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5a8db9-7748-49a2-abcb-890b5108bcd6",
   "metadata": {},
   "source": [
    "üß™ üìä Confusion Matrix for Classification Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcacbd8-f4b1-4fa5-b283-3ca32b59098e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint (only if file exists!)\n",
    "# import os\n",
    "\n",
    "# if os.path.exists('best_dino_checkpoint.pth'):\n",
    "#     checkpoint = torch.load('best_dino_checkpoint.pth')\n",
    "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#     print(f\"‚úÖ Loaded checkpoint from epoch {checkpoint['epoch']} with best acc: {checkpoint['best_acc']:.4f}\")\n",
    "# else:\n",
    "#     print(\"‚ùå No checkpoint file found. Train the model first.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-GPU",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
